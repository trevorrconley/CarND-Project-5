{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Self-Driving Car Engineer Nanodegree\n",
    "## Project: Vehicle Detection and Tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.image as mpimg\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import cv2\n",
    "import glob\n",
    "import time\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from skimage.feature import hog\n",
    "from sklearn.model_selection import train_test_split\n",
    "from skimage.feature import hog\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Features extraction\n",
    "\n",
    "Here we define functions for features extraction (HOG, binned color and color histogram features). The functions are based on code from the Udacity's lectures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_hog_features(img, orient, pix_per_cell, cell_per_block, vis=False, feature_vec=True):\n",
    "    if vis == True: # Call with two outputs if vis==True to visualize the HOG\n",
    "        features, hog_image = hog(img, orientations=orient, \n",
    "                                  pixels_per_cell=(pix_per_cell, pix_per_cell),\n",
    "                                  cells_per_block=(cell_per_block, cell_per_block), \n",
    "                                  transform_sqrt=True, \n",
    "                                  visualise=vis, feature_vector=feature_vec)\n",
    "        return features, hog_image\n",
    "    else:      # Otherwise call with one output\n",
    "        features = hog(img, orientations=orient, \n",
    "                       pixels_per_cell=(pix_per_cell, pix_per_cell),\n",
    "                       cells_per_block=(cell_per_block, cell_per_block), \n",
    "                       transform_sqrt=True, \n",
    "                       visualise=vis, feature_vector=feature_vec)\n",
    "        return features\n",
    "\n",
    "# Define a function to compute binned color features  \n",
    "def bin_spatial(img, size=(16, 16)):\n",
    "    return cv2.resize(img, size).ravel() \n",
    "\n",
    "# Define a function to compute color histogram features \n",
    "def color_hist(img, nbins=32):\n",
    "    ch1 = np.histogram(img[:,:,0], bins=nbins, range=(0, 256))[0]#We need only the histogram, no bins edges\n",
    "    ch2 = np.histogram(img[:,:,1], bins=nbins, range=(0, 256))[0]\n",
    "    ch3 = np.histogram(img[:,:,2], bins=nbins, range=(0, 256))[0]\n",
    "    hist = np.hstack((ch1, ch2, ch3))\n",
    "    return hist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The `extract_features` function extracl all nessesary features from images. It also augment the train dataset by horizontal image flipping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Define a function to extract features from a list of images\n",
    "def img_features(feature_image, spatial_feat, hist_feat, hog_feat, hist_bins, orient, \n",
    "                        pix_per_cell, cell_per_block, hog_channel):\n",
    "    file_features = []\n",
    "    if spatial_feat == True:\n",
    "        spatial_features = bin_spatial(feature_image, size=spatial_size)\n",
    "        #print 'spat', spatial_features.shape\n",
    "        file_features.append(spatial_features)\n",
    "    if hist_feat == True:\n",
    "         # Apply color_hist()\n",
    "        hist_features = color_hist(feature_image, nbins=hist_bins)\n",
    "        #print 'hist', hist_features.shape\n",
    "        file_features.append(hist_features)\n",
    "    if hog_feat == True:\n",
    "    # Call get_hog_features() with vis=False, feature_vec=True\n",
    "        if hog_channel == 'ALL':\n",
    "            hog_features = []\n",
    "            for channel in range(feature_image.shape[2]):\n",
    "                hog_features.append(get_hog_features(feature_image[:,:,channel], \n",
    "                                        orient, pix_per_cell, cell_per_block, \n",
    "                                        vis=False, feature_vec=True))\n",
    "                hog_features = np.ravel(hog_features)        \n",
    "        else:\n",
    "            feature_image = cv2.cvtColor(feature_image, cv2.COLOR_LUV2RGB)\n",
    "            feature_image = cv2.cvtColor(feature_image, cv2.COLOR_RGB2GRAY)\n",
    "            hog_features = get_hog_features(feature_image[:,:], orient, \n",
    "                            pix_per_cell, cell_per_block, vis=False, feature_vec=True)\n",
    "                #print 'hog', hog_features.shape\n",
    "            # Append the new feature vector to the features list\n",
    "        file_features.append(hog_features)\n",
    "    return file_features\n",
    "\n",
    "def extract_features(imgs, color_space='RGB', spatial_size=(32, 32),\n",
    "                        hist_bins=32, orient=9, \n",
    "                        pix_per_cell=8, cell_per_block=2, hog_channel=0,\n",
    "                        spatial_feat=True, hist_feat=True, hog_feat=True):\n",
    "    # Create a list to append feature vectors to\n",
    "    features = []\n",
    "    # Iterate through the list of images\n",
    "    for file_p in imgs:\n",
    "        file_features = []\n",
    "        image = cv2.imread(file_p) # Read in each imageone by one\n",
    "        # apply color conversion if other than 'RGB'\n",
    "        if color_space != 'RGB':\n",
    "            if color_space == 'HSV':\n",
    "                feature_image = cv2.cvtColor(image, cv2.COLOR_RGB2HSV)\n",
    "            elif color_space == 'LUV':\n",
    "                feature_image = cv2.cvtColor(image, cv2.COLOR_RGB2LUV)\n",
    "            elif color_space == 'HLS':\n",
    "                feature_image = cv2.cvtColor(image, cv2.COLOR_RGB2HLS)\n",
    "            elif color_space == 'YUV':\n",
    "                feature_image = cv2.cvtColor(image, cv2.COLOR_RGB2YUV)\n",
    "            elif color_space == 'YCrCb':\n",
    "                feature_image = cv2.cvtColor(image, cv2.COLOR_RGB2YCrCb)\n",
    "        else: feature_image = np.copy(image)      \n",
    "        file_features = img_features(feature_image, spatial_feat, hist_feat, hog_feat, hist_bins, orient, \n",
    "                        pix_per_cell, cell_per_block, hog_channel)\n",
    "        features.append(np.concatenate(file_features))\n",
    "#        feature_image=cv2.flip(feature_image,1) # Augment the dataset with flipped images\n",
    "#        file_features = img_features(feature_image, spatial_feat, hist_feat, hog_feat, hist_bins, orient, \n",
    "#                        pix_per_cell, cell_per_block, hog_channel)\n",
    "#        features.append(np.concatenate(file_features))\n",
    "    return features # Return list of feature vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Data loading\n",
    "Here we create lists of [vehicles](https://s3.amazonaws.com/udacity-sdc/Vehicle_Tracking/vehicles.zip) and [not-vehicles](https://s3.amazonaws.com/udacity-sdc/Vehicle_Tracking/non-vehicles.zip) images provided by Udacity. Corrisponding folders contain unzilled archives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Read in cars and notcars\n",
    "images = glob.glob('*vehicles/*/*')\n",
    "cars = []\n",
    "notcars = []\n",
    "for image in images:\n",
    "    if 'non' in image:\n",
    "        notcars.append(image)\n",
    "    else:\n",
    "        cars.append(image)\n",
    "## Uncomment if you need to reduce the sample size\n",
    "#sample_size = 500\n",
    "#cars = cars[0:sample_size]\n",
    "#notcars = notcars[0:sample_size]\n",
    "print(len(cars))\n",
    "print(len(notcars))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "As we can see, there are about the same number of objects of both classes, so, wo do not need to balance number of images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Classifier\n",
    "\n",
    "The following code chunk creates feature list for the training data. Feature list is normolized by the `StandardScaler()` method from `sklearn`. The data is splitted into thaining and testing subsets (80% and 20%).\n",
    "\n",
    "\n",
    "The classifier (Linear SVM) is trained there as well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Define parameters for feature extraction\n",
    "color_space = 'YUV' # Can be RGB, HSV, LUV, HLS, YUV, YCrCb\n",
    "orient = 8  # HOG orientations\n",
    "pix_per_cell = 8 # HOG pixels per cell\n",
    "cell_per_block = 2 # HOG cells per block\n",
    "hog_channel = 0 # Can be 0, 1, 2, or \"ALL\"\n",
    "spatial_size = (16, 16) # Spatial binning dimensions\n",
    "hist_bins = 32    # Number of histogram bins\n",
    "spatial_feat = True # Spatial features on or off\n",
    "hist_feat = True # Histogram features on or off\n",
    "hog_feat = True # HOG features on or off\n",
    "\n",
    "car_features = extract_features(cars, color_space=color_space, \n",
    "                        spatial_size=spatial_size, hist_bins=hist_bins, \n",
    "                        orient=orient, pix_per_cell=pix_per_cell, \n",
    "                        cell_per_block=cell_per_block, \n",
    "                        hog_channel=hog_channel, spatial_feat=spatial_feat, \n",
    "                        hist_feat=hist_feat, hog_feat=hog_feat)\n",
    "print ('Car samples: ', len(car_features))\n",
    "notcar_features = extract_features(notcars, color_space=color_space, \n",
    "                        spatial_size=spatial_size, hist_bins=hist_bins, \n",
    "                        orient=orient, pix_per_cell=pix_per_cell, \n",
    "                        cell_per_block=cell_per_block, \n",
    "                        hog_channel=hog_channel, spatial_feat=spatial_feat, \n",
    "                        hist_feat=hist_feat, hog_feat=hog_feat)\n",
    "print ('Notcar samples: ', len(notcar_features))\n",
    "X = np.vstack((car_features, notcar_features)).astype(np.float64)                        \n",
    "\n",
    "X_scaler = StandardScaler().fit(X) # Fit a per-column scaler\n",
    "scaled_X = X_scaler.transform(X) # Apply the scaler to X\n",
    "\n",
    "y = np.hstack((np.ones(len(car_features)), np.zeros(len(notcar_features)))) # Define the labels vector\n",
    "\n",
    "# Split up data into randomized training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(scaled_X, y, test_size=0.2, random_state=22)\n",
    "\n",
    "print('Using:',orient,'orientations', pix_per_cell,\n",
    "    'pixels per cell and', cell_per_block,'cells per block')\n",
    "print('Feature vector length:', len(X_train[0]))\n",
    "svc = LinearSVC(loss='hinge') # Use a linear SVC \n",
    "t=time.time() # Check the training time for the SVC\n",
    "svc.fit(X_train, y_train) # Train the classifier\n",
    "t2 = time.time()\n",
    "print(round(t2-t, 2), 'Seconds to train SVC...')\n",
    "print('Test Accuracy of SVC = ', round(svc.score(X_test, y_test), 4)) # Check the score of the SVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Slide window\n",
    "\n",
    "Here we define a sliding window function `slide_window` to generate a list of boxes with predefined parameters and a `draw_boxes` to draw the list of boxes on an image.\n",
    "These and some of the following functions are from the Udacity's lectures because they just work and perform useful tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def slide_window(img, x_start_stop=[None, None], y_start_stop=[None, None], \n",
    "                    xy_window=(64, 64), xy_overlap=(0.5, 0.5)):\n",
    "    # If x and/or y start/stop positions not defined, set to image size\n",
    "    if x_start_stop[0] == None:\n",
    "        x_start_stop[0] = 0\n",
    "    if x_start_stop[1] == None:\n",
    "        x_start_stop[1] = img.shape[1]\n",
    "    if y_start_stop[0] == None:\n",
    "        y_start_stop[0] = 0\n",
    "    if y_start_stop[1] == None:\n",
    "        y_start_stop[1] = img.shape[0]\n",
    "    # Compute the span of the region to be searched    \n",
    "    xspan = x_start_stop[1] - x_start_stop[0]\n",
    "    yspan = y_start_stop[1] - y_start_stop[0]\n",
    "    # Compute the number of pixels per step in x/y\n",
    "    nx_pix_per_step = np.int(xy_window[0]*(1 - xy_overlap[0]))\n",
    "    ny_pix_per_step = np.int(xy_window[1]*(1 - xy_overlap[1]))\n",
    "    # Compute the number of windows in x/y\n",
    "    nx_buffer = np.int(xy_window[0]*(xy_overlap[0]))\n",
    "    ny_buffer = np.int(xy_window[1]*(xy_overlap[1]))\n",
    "    nx_windows = np.int((xspan-nx_buffer)/nx_pix_per_step) \n",
    "    ny_windows = np.int((yspan-ny_buffer)/ny_pix_per_step) \n",
    "    # Initialize a list to append window positions to\n",
    "    window_list = []\n",
    "    # Loop through finding x and y window positions\n",
    "    # Note: you could vectorize this step, but in practice\n",
    "    # you'll be considering windows one by one with your\n",
    "    # classifier, so looping makes sense\n",
    "    for ys in range(ny_windows):\n",
    "        for xs in range(nx_windows):\n",
    "            # Calculate window position\n",
    "            startx = xs*nx_pix_per_step + x_start_stop[0]\n",
    "            endx = startx + xy_window[0]\n",
    "            starty = ys*ny_pix_per_step + y_start_stop[0]\n",
    "            endy = starty + xy_window[1]\n",
    "            # Append window position to list\n",
    "            window_list.append(((startx, starty), (endx, endy)))\n",
    "    # Return the list of windows\n",
    "    return window_list\n",
    "\n",
    "# Define a function to draw bounding boxes on an image\n",
    "def draw_boxes(img, bboxes, color=(0, 0, 255), thick=6):\n",
    "    imcopy = np.copy(img) # Make a copy of the image\n",
    "    for bbox in bboxes: # Iterate through the bounding boxes\n",
    "        cv2.rectangle(imcopy, bbox[0], bbox[1], color, thick)\n",
    "    return imcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def single_img_features(img, color_space='RGB', spatial_size=(32, 32),\n",
    "                        hist_bins=32, orient=9, \n",
    "                        pix_per_cell=8, cell_per_block=2, hog_channel=0,\n",
    "                        spatial_feat=True, hist_feat=True, hog_feat=True):    \n",
    "    #1) Define an empty list to receive features\n",
    "    img_features = []\n",
    "    #2) Apply color conversion if other than 'RGB'\n",
    "    if color_space != 'RGB':\n",
    "        if color_space == 'HSV':\n",
    "            feature_image = cv2.cvtColor(img, cv2.COLOR_RGB2HSV)\n",
    "        elif color_space == 'LUV':\n",
    "            feature_image = cv2.cvtColor(img, cv2.COLOR_RGB2LUV)\n",
    "        elif color_space == 'HLS':\n",
    "            feature_image = cv2.cvtColor(img, cv2.COLOR_RGB2HLS)\n",
    "        elif color_space == 'YUV':\n",
    "            feature_image = cv2.cvtColor(img, cv2.COLOR_RGB2YUV)\n",
    "        elif color_space == 'YCrCb':\n",
    "            feature_image = cv2.cvtColor(img, cv2.COLOR_RGB2YCrCb)\n",
    "    else: feature_image = np.copy(img)      \n",
    "    #3) Compute spatial features if flag is set\n",
    "    if spatial_feat == True:\n",
    "        spatial_features = bin_spatial(feature_image, size=spatial_size)\n",
    "        #4) Append features to list\n",
    "        img_features.append(spatial_features)\n",
    "    #5) Compute histogram features if flag is set\n",
    "    if hist_feat == True:\n",
    "        hist_features = color_hist(feature_image, nbins=hist_bins)\n",
    "        #6) Append features to list\n",
    "        img_features.append(hist_features)\n",
    "    #7) Compute HOG features if flag is set\n",
    "    if hog_feat == True:\n",
    "        if hog_channel == 'ALL':\n",
    "            hog_features = []\n",
    "            for channel in range(feature_image.shape[2]):\n",
    "                hog_features.extend(get_hog_features(feature_image[:,:,channel], \n",
    "                                    orient, pix_per_cell, cell_per_block, \n",
    "                                    vis=False, feature_vec=True))      \n",
    "        else:\n",
    "            hog_features = get_hog_features(feature_image[:,:,hog_channel], orient, \n",
    "                        pix_per_cell, cell_per_block, vis=False, feature_vec=True)\n",
    "        #8) Append features to list\n",
    "        img_features.append(hog_features)\n",
    "    #9) Return concatenated array of features\n",
    "    return np.concatenate(img_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Define a function you will pass an image \n",
    "# and the list of windows to be searched (output of slide_windows())\n",
    "def search_windows(img, windows, clf, scaler, color_space='RGB', \n",
    "                    spatial_size=(32, 32), hist_bins=32, \n",
    "                    hist_range=(0, 256), orient=8, \n",
    "                    pix_per_cell=8, cell_per_block=2, \n",
    "                    hog_channel=0, spatial_feat=True, \n",
    "                    hist_feat=True, hog_feat=True):\n",
    "\n",
    "    #1) Create an empty list to receive positive detection windows\n",
    "    on_windows = []\n",
    "    #2) Iterate over all windows in the list\n",
    "    for window in windows:\n",
    "        #3) Extract the test window from original image\n",
    "        test_img = cv2.resize(img[window[0][1]:window[1][1], window[0][0]:window[1][0]], (64, 64))      \n",
    "        #4) Extract features for that window using single_img_features()\n",
    "        features = single_img_features(test_img, color_space=color_space, \n",
    "                            spatial_size=spatial_size, hist_bins=hist_bins, \n",
    "                            orient=orient, pix_per_cell=pix_per_cell, \n",
    "                            cell_per_block=cell_per_block, \n",
    "                            hog_channel=hog_channel, spatial_feat=spatial_feat, \n",
    "                            hist_feat=hist_feat, hog_feat=hog_feat)\n",
    "        #5) Scale extracted features to be fed to classifier\n",
    "        test_features = scaler.transform(np.array(features).reshape(1, -1))\n",
    "        #6) Predict using your classifier\n",
    "        prediction = clf.predict(test_features)\n",
    "        #7) If positive (prediction == 1) then save the window\n",
    "        if prediction == 1:\n",
    "            on_windows.append(window)\n",
    "    #8) Return windows for positive detections\n",
    "    return on_windows\n",
    "\n",
    "# A function to show an image\n",
    "def show_img(img):\n",
    "    if len(img.shape)==3: #Color BGR image\n",
    "        plt.figure()\n",
    "        plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
    "    else: # Grayscale image\n",
    "        plt.figure()\n",
    "        plt.imshow(img, cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Classifier test\n",
    "\n",
    "Here we test the calssifier on the test images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "t=time.time() # Start time\n",
    "for image_p in glob.glob('test_images/test*.jpg'):\n",
    "    image = cv2.imread(image_p)\n",
    "    draw_image = np.copy(image)\n",
    "    windows = slide_window(image, x_start_stop=[None, None], y_start_stop=[400, 640], \n",
    "                    xy_window=(128, 128), xy_overlap=(0.85, 0.85))\n",
    "    hot_windows = []\n",
    "    hot_windows += (search_windows(image, windows, svc, X_scaler, color_space=color_space, \n",
    "                        spatial_size=spatial_size, hist_bins=hist_bins, \n",
    "                        orient=orient, pix_per_cell=pix_per_cell, \n",
    "                        cell_per_block=cell_per_block, \n",
    "                        hog_channel=hog_channel, spatial_feat=spatial_feat, \n",
    "                        hist_feat=hist_feat, hog_feat=hog_feat))                       \n",
    "    window_img = draw_boxes(draw_image, hot_windows, color=(0, 0, 255), thick=6)                    \n",
    "    show_img(window_img)\n",
    "print(round(time.time()-t, 2), 'Seconds to process test images')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "As we can see on examples above, the classifier successfully finds cars on the test images. However, there is a false positive example, so, we will need to apply a kind of filter (such as heat map) and the classifier failed to find a car on th 3rd image because it is too small for it. That is why, we will need to use multi scale windows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Advanced Sliding Windows\n",
    "\n",
    "To increase performance we need to analize the smallest possible number of windows. That is why, we will scan with a search window not across the whole image, but only areas where a new car can appear and also we are going to scan areas where a car was detected (track cars). \n",
    "### Detect new cars\n",
    "On every frame we look for new passing cars (red areas on sides) cars and new far cars (blue area)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "image = cv2.imread('test_images/test2.jpg')\n",
    "windows = slide_window(image, x_start_stop=[930, None], y_start_stop=[420, 650], \n",
    "                    xy_window=(128, 128), xy_overlap=(0.75, 0.75))\n",
    "windows += slide_window(image, x_start_stop=[0, 350], y_start_stop=[420, 650], \n",
    "                    xy_window=(128, 128), xy_overlap=(0.75, 0.75))\n",
    "window_img = draw_boxes(image, windows, color=(0, 0, 255), thick=6) \n",
    "windows = slide_window(image, x_start_stop=[400, 880], y_start_stop=[400, 470], \n",
    "                    xy_window=(48, 48), xy_overlap=(0.75, 0.75))\n",
    "window_img = draw_boxes(window_img, windows, color=(255, 0, 0), thick=6)                    \n",
    "show_img(window_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Refine detected car position\n",
    "\n",
    "For every detected car we are doing to scan with a sliding window the ROI around the previous known position. We use multiple scales of windows in order to detect the car and its position more accurate and reliable.\n",
    "\n",
    "Here is an example with two scales of windows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\n",
    "image = cv2.imread('test_images/test5.jpg')\n",
    "track = (880, 450)\n",
    "w_size = 80\n",
    "windows = slide_window(image, x_start_stop=[track[0]-w_size,track[0]+w_size], \n",
    "                       y_start_stop=[track[1]-w_size,track[1]+w_size], \n",
    "                       xy_window=(128, 128), xy_overlap=(0.75, 0.75))\n",
    "window_img = draw_boxes(image, windows, color=(0, 0, 255), thick=6)\n",
    "windows = slide_window(image, x_start_stop=[track[0]-w_size,track[0]+w_size], \n",
    "                       y_start_stop=[track[1]-int(w_size),track[1]+int(w_size)], \n",
    "                       xy_window=(48, 48), xy_overlap=(0.75, 0.75))\n",
    "window_img = draw_boxes(window_img, windows, color=(255, 0, 0), thick=6)                    \n",
    "show_img(window_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The following code chunk find windows with a car in a given range with windows of a given scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def convert_color(img):\n",
    "    return cv2.cvtColor(img, cv2.COLOR_RGB2YUV)\n",
    "\n",
    "def find_cars(img, ystart, ystop, xstart, xstop, scale, step):\n",
    "    boxes = []\n",
    "    draw_img = np.zeros_like(img)   \n",
    "    img_tosearch = img[int(ystart):int(ystop),int(xstart):int(xstop),:]\n",
    "    ctrans_tosearch = convert_color(img_tosearch)\n",
    "    if scale != 1:\n",
    "        imshape = ctrans_tosearch.shape\n",
    "        ctrans_tosearch = cv2.resize(ctrans_tosearch, (np.int(imshape[1]/scale), np.int(imshape[0]/scale)))       \n",
    "    ch1 = ctrans_tosearch[:,:,0]\n",
    "    ch2 = ctrans_tosearch[:,:,1]\n",
    "    ch3 = ctrans_tosearch[:,:,2]\n",
    "    # Define blocks and steps as above\n",
    "    nxblocks = (ch1.shape[1] // pix_per_cell)-1\n",
    "    nyblocks = (ch1.shape[0] // pix_per_cell)-1 \n",
    "    nfeat_per_block = orient*cell_per_block**2\n",
    "    # 64 was the orginal sampling rate, with 8 cells and 8 pix per cell\n",
    "    window = 64\n",
    "    nblocks_per_window = (window // pix_per_cell) -1\n",
    "    cells_per_step = step  # Instead of overlap, define how many cells to step\n",
    "    nxsteps = (nxblocks - nblocks_per_window) // cells_per_step\n",
    "    nysteps = (nyblocks - nblocks_per_window) // cells_per_step\n",
    "    # Compute individual channel HOG features for the entire image\n",
    "    hog1 = get_hog_features(ch1, orient, pix_per_cell, cell_per_block, feature_vec=False)\n",
    "    for xb in range(nxsteps):\n",
    "        for yb in range(nysteps):\n",
    "            ypos = yb*cells_per_step\n",
    "            xpos = xb*cells_per_step\n",
    "            # Extract HOG for this patch\n",
    "            hog_features = hog1[ypos:ypos+nblocks_per_window, xpos:xpos+nblocks_per_window].ravel() \n",
    "            xleft = xpos*pix_per_cell\n",
    "            ytop = ypos*pix_per_cell\n",
    "            # Extract the image patch\n",
    "            subimg = ctrans_tosearch[ytop:ytop+window, xleft:xleft+window]\n",
    "            # Get color features\n",
    "            spatial_features = bin_spatial(subimg, size=spatial_size)\n",
    "            hist_features = color_hist(subimg, nbins=hist_bins)\n",
    "            # Scale features and make a prediction\n",
    "            test_features = X_scaler.transform(np.hstack((spatial_features, hist_features, hog_features)).reshape(1, -1))        \n",
    "            test_prediction = svc.predict(test_features)\n",
    "            if test_prediction == 1:\n",
    "                xbox_left = np.int(xleft*scale)+xstart\n",
    "                ytop_draw = np.int(ytop*scale)\n",
    "                win_draw = np.int(window*scale)\n",
    "                boxes.append(((int(xbox_left), int(ytop_draw+ystart)),(int(xbox_left+win_draw),int(ytop_draw+win_draw+ystart))))\n",
    "    return boxes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Frames processing\n",
    "\n",
    "Here we process individual images or videos. To increase performance we skip every 2nd frame because we do not expect very fast moving of the detected cars. We filter all found windows by a heatmap approach (with *THRES* threshold), suggested in lectures.\n",
    "\n",
    "In order to reduce jitter a function `filt` applies a simple low-pass filter on the new and the previous cars boxes coordinates and sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from scipy.ndimage.measurements import label\n",
    "\n",
    "THRES = 5 # Minimal overlapping boxes\n",
    "ALPHA = 0.75 # Filter parameter, weight of the previous measurements\n",
    "\n",
    "image = cv2.imread('test_images/test1.jpg')\n",
    "track_list = []#[np.array([880, 440, 76, 76])]\n",
    "#track_list += [np.array([1200, 480, 124, 124])]\n",
    "THRES_LEN = 32\n",
    "Y_MIN = 400\n",
    "\n",
    "heat_p = np.zeros((720, 1280)) # Store prev heat image\n",
    "boxes_p = [] # Store prev car boxes\n",
    "n_count = 0 # Frame counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def add_heat(heatmap, bbox_list):\n",
    "    # Iterate through list of bboxes\n",
    "    for box in bbox_list:\n",
    "        # Add += 1 for all pixels inside each bbox\n",
    "        # Assuming each \"box\" takes the form ((x1, y1), (x2, y2))\n",
    "        heatmap[box[0][1]:box[1][1], box[0][0]:box[1][0]] += 1\n",
    "    return heatmap # Return updated heatmap\n",
    "    \n",
    "def apply_threshold(heatmap, threshold): # Zero out pixels below the threshold in the heatmap\n",
    "    heatmap[heatmap < threshold] = 0 \n",
    "    return heatmap \n",
    "\n",
    "def filt(a,b,alpha): # Smooth the car boxes\n",
    "    return a*alpha+(1.0-alpha)*b\n",
    "\n",
    "def len_points(p1, p2): # Distance beetween two points\n",
    "    return np.sqrt((p1[0]-p2[0])**2+(p1[1]-p2[1])**2)\n",
    "\n",
    "def track_to_box(p): # Create box coordinates out of its center and span\n",
    "    return ((int(p[0]-p[2]),int(p[1]-p[3])),(int(p[0]+p[2]), int(p[1]+p[3])))\n",
    "\n",
    "\n",
    "def draw_labeled_bboxes(labels):\n",
    "    global track_list\n",
    "    track_list_l = []\n",
    "    for car_number in range(1, labels[1]+1):\n",
    "        # Find pixels with each car_number label value\n",
    "        nonzero = (labels[0] == car_number).nonzero()\n",
    "        # Identify x and y values of those pixels\n",
    "        nonzeroy = np.array(nonzero[0])\n",
    "        nonzerox = np.array(nonzero[1])\n",
    "        # Define a bounding box based on min/max x and y\n",
    "        bbox = ((np.min(nonzerox), np.min(nonzeroy)), (np.max(nonzerox), np.max(nonzeroy)))\n",
    "        #img = draw_boxes(np.copy(img), [bbox], color=(255,0,255), thick=3)\n",
    "        size_x = (bbox[1][0]-bbox[0][0])/2.0 #Size of the found box\n",
    "        size_y = (bbox[1][1]-bbox[0][1])/2.0\n",
    "        asp_d = size_x / size_y\n",
    "        size_m = (size_x + size_y)/2\n",
    "        x = size_x+bbox[0][0]\n",
    "        y = size_y+bbox[0][1]\n",
    "        asp = (y-Y_MIN)/130.0+1.25 # Best rectangle aspect ratio for the box (coefficients from perspectieve measurements and experiments)\n",
    "        if x>1050 or x<230:\n",
    "            asp*=1.4\n",
    "        asp = max(asp, asp_d) # for several cars chunk\n",
    "        size_ya = np.sqrt(size_x*size_y/asp)\n",
    "        size_xa = int(size_ya*asp)\n",
    "        size_ya = int(size_ya)\n",
    "        if x > (-3.049*y+1809): #If the rectangle on the road, coordinates estimated from a test image\n",
    "            track_list_l.append(np.array([x, y, size_xa, size_ya]))\n",
    "            if len(track_list) > 0:\n",
    "                track_l = track_list_l[-1]\n",
    "                dist = []\n",
    "                for track in track_list:\n",
    "                    dist.append(len_points(track, track_l))\n",
    "                min_d = min(dist)\n",
    "                if min_d < THRES_LEN:\n",
    "                    ind = dist.index(min_d)\n",
    "                    track_list_l[-1] = filt(track_list[ind], track_list_l[-1], ALPHA)\n",
    "    track_list = track_list_l\n",
    "    boxes = []\n",
    "    for track in track_list_l:\n",
    "        #print(track_to_box(track))\n",
    "        boxes.append(track_to_box(track))\n",
    "    return boxes\n",
    "\n",
    "def frame_proc(img, lane = False, video = False, vis = False):\n",
    "    if (video and n_count%2==0) or not video: # Skip every second video frame\n",
    "        global heat_p, boxes_p, n_count\n",
    "        heat = np.zeros_like(img[:,:,0]).astype(np.float)\n",
    "        boxes = []\n",
    "        boxes = find_cars(img, Y_MIN, 650, 950, 1280, 2.0, 1)\n",
    "        boxes += find_cars(img, Y_MIN, 500, 950, 1280, 1.5, 1)\n",
    "        boxes += find_cars(img, Y_MIN, 650, 0, 330, 2.0, 1)\n",
    "        boxes += find_cars(img, Y_MIN, 500, 0, 330, 1.5, 1)\n",
    "        boxes += find_cars(img, Y_MIN, 460, 330, 950, 0.75, 1)\n",
    "        for track in track_list:\n",
    "            y_loc = track[1]+track[3]\n",
    "            lane_w = (y_loc*2.841-1170.0)/3.0\n",
    "            if lane_w < 96:\n",
    "                lane_w = 96\n",
    "            lane_h = lane_w/1.2\n",
    "            lane_w = max(lane_w, track[2])\n",
    "            xs = track[0]-lane_w\n",
    "            xf = track[0]+lane_w\n",
    "            if track[1] < Y_MIN:\n",
    "                track[1] = Y_MIN\n",
    "            ys = track[1]-lane_h\n",
    "            yf = track[1]+lane_h\n",
    "            if xs < 0: xs=0\n",
    "            if xf > 1280: xf=1280\n",
    "            if ys < Y_MIN - 40: ys=Y_MIN - 40\n",
    "            if yf > 720: yf=720\n",
    "            size_sq = lane_w / (0.015*lane_w+0.3)\n",
    "            scale = size_sq / 64.0\n",
    "            # Apply multi scale image windows \n",
    "            boxes+=find_cars(img, ys, yf, xs, xf, scale, 1)\n",
    "            boxes+=find_cars(img, ys, yf, xs, xf, scale*1.25, 1)\n",
    "            boxes+=find_cars(img, ys, yf, xs, xf, scale*1.5, 1)\n",
    "            boxes+=find_cars(img, ys, yf, xs, xf, scale*1.75, 1)\n",
    "            if vis:\n",
    "                cv2.rectangle(img, (int(xs), int(ys)), (int(xf), int(yf)), color=(0,255,0), thickness=3)\n",
    "        heat = add_heat(heat, boxes)\n",
    "        heat_l = heat_p + heat\n",
    "        heat_p = heat\n",
    "        heat_l = apply_threshold(heat_l,THRES) # Apply threshold to help remove false positives\n",
    "        # Visualize the heatmap when displaying    \n",
    "        heatmap = np.clip(heat_l, 0, 255)\n",
    "#        show_img(heatmap)\n",
    "        # Find final boxes from heatmap using label function\n",
    "        labels = label(heatmap)\n",
    "        #print((labels[0]))\n",
    "        cars_boxes = draw_labeled_bboxes(labels)\n",
    "        boxes_p = cars_boxes\n",
    "        \n",
    "    else:\n",
    "        cars_boxes = boxes_p\n",
    "    if lane: #If we was asked to draw the lane line, do it\n",
    "        if video:\n",
    "            img = laneline.draw_lane(img, True)\n",
    "        else:\n",
    "            img = laneline.draw_lane(img, False)\n",
    "    imp = draw_boxes(np.copy(img), cars_boxes, color=(0, 0, 255), thick=6)\n",
    "    if vis:\n",
    "        imp = draw_boxes(imp, boxes, color=(0, 255, 255), thick=2)\n",
    "        for track in track_list:\n",
    "            cv2.circle(imp, (int(track[0]), int(track[1])), 5, color=(255, 0, 255), thickness=4)\n",
    "    n_count += 1\n",
    "    return imp\n",
    "\n",
    "show_img(frame_proc(image, lane=False, vis=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Video processing\n",
    "\n",
    "Here we process all three task videos of the project.\n",
    "\n",
    "Video processing approach with `moviepy` from the [first project](https://github.com/NikolasEnt/LaneLine-project) of the Udacity Self-Driving Car Nanodegreee."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from moviepy.editor import VideoFileClip\n",
    "n_count = 0\n",
    "#laneline.init_params(0.0)\n",
    "def process_image(image):\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "    return cv2.cvtColor(frame_proc(image, lane=False, video=True, vis=False), cv2.COLOR_BGR2RGB)\n",
    "\n",
    "output_v = 'project_video_proc.mp4'\n",
    "clip1 = VideoFileClip(\"project_video.mp4\")\n",
    "clip = clip1.fl_image(process_image)\n",
    "%time clip.write_videofile(output_v, audio=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "\n",
    "HTML(\"\"\"\n",
    "<video width=\"800\" height=\"640\" controls>\n",
    "  <source src=\"project_output.mp4\" type=\"video/mp4\">\n",
    "</video>\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
